# Synthetic Speech Detection in Video

Download [lipsync_v4_73.mat](https://www.dropbox.com/sh/5ecqtwuy85nlonb/AADVrCJZzvhmzAc1jKh0a1TBa/lipsync_v4_73.mat) and [shape_predictor_68_face_landmarks.dat]( https://github.com/GuoQuanhao/68_points/raw/master/shape_predictor_68_face_landmarks.dat)

This Project is forked from this [Repository](https://github.com/Neha13022020/Syncnet_model_VIDTIMIT_dataset) <br>
Follow this [Blog](https://medium.com/@nehasikerwar/syncnet-model-with-vidtimit-dataset-dd9de2cb2fb5) for step by step explanation.

Research paper refered throughout this project: https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf <br>
They focused on determining the audio-video synchronization between mouth motion and speech in the video. They used audio-video synchronization for TV broadcasting. It's really a nice research paper, they developed a language-independent and speaker-independent solution to the lip-sync problem, without labeled data. 

For the modeling and processing functions: https://github.com/voletiv/syncnet-in-keras <br>
VidTIMIT dataset used in this project: http://conradsanderson.id.au/vidtimit
